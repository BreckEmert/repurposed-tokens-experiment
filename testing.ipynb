{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12babc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.7.0+cu118)\n",
      "    Python  3.10.11 (you have 3.10.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "PyTorch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU count: 1\n",
      "Device name: NVIDIA GeForce RTX 4080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPyTorch version: 2.7.0+cu118\\nCUDA available: True\\nCUDA version: 11.8\\nGPU count: 1\\nDevice name: NVIDIA GeForce RTX 4080\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import unsloth\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import os, json, re, sys, tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# --- GPU check ---\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "\"\"\"\n",
    "PyTorch version: 2.7.0+cu118\n",
    "CUDA available: True\n",
    "CUDA version: 11.8\n",
    "GPU count: 1\n",
    "Device name: NVIDIA GeForce RTX 4080\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the model ---\n",
    "\n",
    "# https://huggingface.co/unsloth/llama-3-8b-bnb-4bit/blob/main/README.md\n",
    "model_id = 'unsloth/llama-3-8b-bnb-4bit'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")  # 5.70 GB\n",
    "print(f\"Memory reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")  # 7.36 GB\n",
    "\n",
    "\"\"\"\n",
    "f:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\accelerate\\utils\\modeling.py:808: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
    "  _ = torch.tensor([0], device=i)\n",
    "Memory allocated: 5.72 GB\n",
    "Memory reserved:  7.36 GB\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the dataset ---\n",
    "\n",
    "gsm = load_dataset('gsm8k', 'main', split='test[:30]')\n",
    "\n",
    "# --- Separator tokens for injection ---\n",
    "\n",
    "SEPARATORS = [\n",
    "    \"\\n\",\n",
    "    \",\",\n",
    "    \".\",\n",
    "    \"and\",\n",
    "    \"the\",\n",
    "    \"about\",\n",
    "    \"neuroplasticity\",\n",
    "    \"metaphor\",\n",
    "    \"paradigm\",\n",
    "    \"operator\"\n",
    "]\n",
    "\n",
    "def inject_separator(prompt: str, sep: str) -> str:\n",
    "    \"\"\"Inserts `sep` between every word in the `prompt`\"\"\"\n",
    "    return f' {sep} '.join(prompt.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt function ---\n",
    "\n",
    "SYSTEM = [{\n",
    "    'role': 'system', \n",
    "    'content': (\n",
    "        'You are a strict math grader.  For each problem return ONLY the final numeric answer'\n",
    "        ', no units, no explanation, no punctuation.'\n",
    "    )\n",
    "}]\n",
    "FEWSHOT = [\n",
    "    {'role': 'user', 'content': 'What is 2 + 3?'},\n",
    "    {'role': 'assistant', 'content': '5'},\n",
    "]\n",
    "TERMINATORS = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "tokenizer.chat_template = (\n",
    "    \"<|begin_of_text|>\\n\"\n",
    "    \"{% for message in messages -%}\"\n",
    "    \"<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n\"\n",
    "    \"{{ message['content'] }}<|eot_id|>\\n\"\n",
    "    \"{% endfor -%}\"\n",
    "    \"{% if add_generation_prompt -%}\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \"{% endif -%}\"\n",
    ")\n",
    "\n",
    "def prepare_messages(user_prompt: str):\n",
    "    messages = SYSTEM + FEWSHOT + [{'role': 'user', 'content': user_prompt}]\n",
    "    full_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return full_prompt\n",
    "\n",
    "def query_model(prompt, model, tokenizer, max_new_tokens=32):\n",
    "    # Standardize prompt input format\n",
    "    full_prompt = prepare_messages(prompt)\n",
    "\n",
    "    # Pass the prompt through the model\n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=TERMINATORS\n",
    "        )\n",
    "    \n",
    "    # Decode only newly generated text\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]  # Only new tokens\n",
    "    output = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "    return output.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sanity checks ---\n",
    "# The model already has a quantization config:\n",
    "# print(model.config.quantization_config)\n",
    "\"\"\"\n",
    "BitsAndBytesConfig {\n",
    "  \"_load_in_4bit\": true,\n",
    "  \"_load_in_8bit\": false,\n",
    "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
    "  \"bnb_4bit_quant_type\": \"nf4\",\n",
    "  \"bnb_4bit_use_double_quant\": true,\n",
    "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
    "  \"llm_int8_has_fp16_weight\": false,\n",
    "  \"llm_int8_skip_modules\": null,\n",
    "  \"llm_int8_threshold\": 6.0,\n",
    "  \"load_in_4bit\": true,\n",
    "  \"load_in_8bit\": false,\n",
    "  \"quant_method\": \"bitsandbytes\"\n",
    "}\n",
    "\"\"\"\n",
    "print(type(model.model.layers[0].self_attn.q_proj))  # bitsandbytes.nn.modules.Linear4bit\n",
    "\n",
    "# Checking terminating tokens\n",
    "ids = {s: tokenizer.convert_tokens_to_ids(s) for s in\n",
    "       [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]}\n",
    "assert all(v != tokenizer.unk_token_id for v in ids.values()), \"one of the tags is not in the vocab\"\n",
    "       # No error; we're good\n",
    "\n",
    "print('Terminators: ', TERMINATORS)  # [128001, 128009]\n",
    "\n",
    "# Check prompt generation\n",
    "messages = [\n",
    "    {\"role\": \"user\",   \"content\": \"What is 9x8?\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is 17+5?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"\\nGenerated prompts:\")\n",
    "print(prompt.strip())\n",
    "\"\"\"\n",
    "<class 'bitsandbytes.nn.modules.Linear4bit'>\n",
    "Terminators:  [128001, 128009]\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a math tutor.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is 17+5?<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Quick output checks\n",
    "print(\"\\n\\nQuick output checks:\")\n",
    "for q in [\"What is 2+2?\", gsm[0][\"question\"]]:\n",
    "    print(q, \"â†’\", query_model(q, model, tokenizer, max_new_tokens=32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation function ---\n",
    "\n",
    "def extract_answer(text):\n",
    "    # Extracting negative signs and decimals\n",
    "    numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', text)\n",
    "    return numbers[-1] if numbers else None\n",
    "    \n",
    "def evaluate(model_output, gsm_answer):\n",
    "    model_answer = extract_answer(model_output)\n",
    "    gsm_answer = extract_answer(gsm_answer)\n",
    "\n",
    "    return model_answer == gsm_answer, model_answer, gsm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70049ea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 2+2?\u001b[39m\u001b[38;5;124m\"\u001b[39m, model, tokenizer))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m gsm\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)):\n\u001b[0;32m      4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print(query_model(\"What is 2+2?\", model, tokenizer))\n",
    "\n",
    "for ex in gsm.select(range(5)):\n",
    "    prompt = ex['question'].strip()\n",
    "    model_output = query_model(prompt, model, tokenizer)\n",
    "    print('Prompt: ', prompt)\n",
    "    print('Output: ', model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1a0ea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_id  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\__init__.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     30\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     logging,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs_doc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     ClassAttrs,\n\u001b[0;32m     26\u001b[0m     ClassDocstring,\n\u001b[0;32m     27\u001b[0m     ImageProcessorArgs,\n\u001b[0;32m     28\u001b[0m     ModelArgs,\n\u001b[0;32m     29\u001b[0m     auto_class_docstring,\n\u001b[0;32m     30\u001b[0m     auto_docstring,\n\u001b[0;32m     31\u001b[0m     parse_docstring,\n\u001b[0;32m     32\u001b[0m     set_min_indent,\n\u001b[0;32m     33\u001b[0m     source_args_doc,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\utils\\args_doc.py:30\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     MODELS_TO_PIPELINE,\n\u001b[0;32m     26\u001b[0m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     27\u001b[0m     PT_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     28\u001b[0m     _prepare_output_docstrings,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[0;32m     33\u001b[0m PATH_TO_TRANSFORMERS \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m AUTODOC_FILES \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m ]\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\utils\\generic.py:46\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     get_torch_version,\n\u001b[0;32m     36\u001b[0m     is_flax_available,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     is_torch_fx_proxy,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcached_property\u001b[39;00m(\u001b[38;5;28mproperty\u001b[39m):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Descriptor that mimics @property but caches output in member variable.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    Built-in in functools from Python 3.8.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\torch\\__init__.py:367\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m    366\u001b[0m         _load_global_deps()\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id  = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Build prompt with builtâ€‘in chat template\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a minimalist assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"Give me a haiku about the moon.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenise & move to GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "out_ids = model.generate(**inputs, max_new_tokens=64, eos_token_id=eot_id)\n",
    "\n",
    "# Decode only the new tokens (assistant reply)\n",
    "reply = tokenizer.decode(out_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f75fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WithPadding Â (crashes)\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "LM (mlm=False) Â â€“ labels overwritten\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:778\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 778\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:740\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 3)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValueError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# B) OK but labels were replaced\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLM (mlm=False) Â â€“ labels overwritten\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m     \u001b[49m\u001b[43mDataCollatorForLanguageModeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# C) OK â€“ pads labels with â€‘100, keeps original mask\u001b[39;00m\n\u001b[0;32m     32\u001b[0m show(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq2Seq Â â€“ just right\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m      DataCollatorForSeq2Seq(tok, label_pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(name, collator)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(name, collator):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcollator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\data\\data_collator.py:46\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\data\\data_collator.py:1013\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[1;34m(self, examples)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_rng()\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m-> 1013\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1017\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[0;32m   1019\u001b[0m     }\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\data\\data_collator.py:67\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3386\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3383\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3384\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:242\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    238\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:794\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    790\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    793\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok.pad_token = tok.eos_token = \"[PAD]\"\n",
    "\n",
    "# Two examples of different length with answerâ€‘only labels\n",
    "ex0 = {\"input_ids\": [101, 2000, 999, 102],               # [CLS go ? SEP]\n",
    "       \"labels\":    [-100, 999, 102, 102]}\n",
    "ex1 = {\"input_ids\": [101, 2023, 2742],              # [CLS this great SEP]\n",
    "       \"labels\":    [-100, -100, 2742]}\n",
    "batch = [ex0, ex1]\n",
    "\n",
    "def show(name, collator):\n",
    "    print(f\"\\n{name}\")\n",
    "    out = collator(batch)\n",
    "    print(\"input_ids:\\n\", out[\"input_ids\"])\n",
    "    print(\"labels:\\n\", out[\"labels\"])\n",
    "\n",
    "# A) crashes because labels remain ragged\n",
    "try:\n",
    "    show(\"WithPadding Â (crashes)\", DataCollatorWithPadding(tok, return_tensors=\"pt\"))\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)\n",
    "\n",
    "# B) OK but labels were replaced\n",
    "show(\"LM (mlm=False) Â â€“ labels overwritten\",\n",
    "     DataCollatorForLanguageModeling(tok, mlm=False))\n",
    "\n",
    "# C) OK â€“ pads labels with â€‘100, keeps original mask\n",
    "show(\"Seq2Seq Â â€“ just right\",\n",
    "     DataCollatorForSeq2Seq(tok, label_pad_token_id=-100, padding=\"longest\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id = 0\n",
      "\n",
      "DataCollatorWithPadding | input_ids\n",
      "tensor([[7592, 2088],\n",
      "        [7632,    0]]) \n",
      "\n",
      "DataCollatorWithPadding | NO labels generated automatically\n",
      "DataCollatorForLanguageModeling | labels\n",
      "tensor([[7592, 2088],\n",
      "        [7632, -100]])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pad_id = tok.pad_token_id\n",
    "tok.pad_token_type_id = -100\n",
    "\n",
    "# Two unequal-length sequences\n",
    "ex1 = tok(\"hello world\", add_special_tokens=False)[\"input_ids\"]  # [7592, 2088]\n",
    "ex2 = tok(\"hi\",           add_special_tokens=False)[\"input_ids\"]  # [763]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Collator your notebook currently uses\n",
    "# ------------------------------------------------------------\n",
    "features = [{\"input_ids\": ex1}, {\"input_ids\": ex2}]\n",
    "pad_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "batch_pad = pad_collator(features)                       # works, pads with pad_id\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Collator you want for causal-LM fine-tuning\n",
    "#    (DONâ€™T pass 'labels'; it builds them itself)\n",
    "# ------------------------------------------------------------\n",
    "lm_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "batch_lm = lm_collator(features)                         # works, masks with -100\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Show the result\n",
    "# ------------------------------------------------------------\n",
    "print(f\"pad_token_id = {pad_id}\\n\")\n",
    "\n",
    "print(\"DataCollatorWithPadding | input_ids\")\n",
    "print(batch_pad[\"input_ids\"], \"\\n\")        # padded positions = pad_id\n",
    "\n",
    "print(\"DataCollatorWithPadding | NO labels generated automatically\")\n",
    "\n",
    "print(\"DataCollatorForLanguageModeling | labels\")\n",
    "print(batch_lm[\"labels\"])                  # padded positions = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25f109b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute 'pad_token_type_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m pad_id \u001b[38;5;241m=\u001b[39m tok\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_type_id\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Has NO effect on labels or input_ids\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Two unequal-length sequences with labels (for padding)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ex1 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m7592\u001b[39m, \u001b[38;5;241m2088\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m7592\u001b[39m, \u001b[38;5;241m2088\u001b[39m]}\n",
      "File \u001b[1;32mf:\\miniforge3\\envs\\repurposed-tokens\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1087\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__setattr__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_special_tokens_map[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1087\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: can't set attribute 'pad_token_type_id'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pad_id = tok.pad_token_id\n",
    "tok.pad_token_type_id = -100  # Has NO effect on labels or input_ids\n",
    "\n",
    "# Two unequal-length sequences with labels (for padding)\n",
    "ex1 = {\"input_ids\": [7592, 2088], \"labels\": [7592, 2088]}\n",
    "ex2 = {\"input_ids\": [763],        \"labels\": [763]}\n",
    "\n",
    "features = [ex1, ex2]\n",
    "\n",
    "# DataCollatorWithPadding\n",
    "pad_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "batch_pad = pad_collator(features)\n",
    "\n",
    "print(\"input_ids:\")\n",
    "print(batch_pad[\"input_ids\"])\n",
    "print(\"labels:\")\n",
    "print(batch_pad[\"labels\"])\n",
    "print(\"token_type_ids (should be padded with -100):\")\n",
    "print(batch_pad.get(\"token_type_ids\"))\n",
    "\n",
    "# DataCollatorForLanguageModeling\n",
    "lm_collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, return_tensors=\"pt\")\n",
    "batch_lm = lm_collator([{\"input_ids\": [7592, 2088]}, {\"input_ids\": [763]}])\n",
    "print(\"DataCollatorForLanguageModeling | labels\")\n",
    "print(batch_lm[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9375d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token_type_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repurposed-tokens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
